{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b765d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\" \",engine='python',encoding='cp1252',na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where 'content' column has blanks\n",
    "df = df[df['content'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ff3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Function to check if a string contains only punctuation\n",
    "def is_only_punctuation(text):\n",
    "    return all(char in string.punctuation for char in text)\n",
    "# Remove rows where 'content' contains only punctuation\n",
    "df = df[~df['content'].apply(is_only_punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special characters and punctuations\n",
    "import regex\n",
    "df['content'] = df['content'].str.replace('[^\\w\\s]','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0327c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, stopwords from 'content' and lemmatize text\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "data = df.content.values.tolist()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#Add custom stop words in the list; for example stop_words.extend(['student', 'read', 'write'])\n",
    "stop_words.extend([])\n",
    "import spacy \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags and len(token)>3])\n",
    "    return texts_out\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'] )\n",
    "print(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52990ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty lists\n",
    "filtered_list = [x for x in data_lemmatized if x]\n",
    "# Create bow corpus\n",
    "dictionary = gensim.corpora.Dictionary(data_lemmatized)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal no. of topics using Coherence score\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.LdaMulticore(bow_corpus, num_topics=num_topics, id2word=dictionary,random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=filtered_list, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=filtered_list, start=2, limit=40, step=1)\n",
    "limit=40; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b10fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21688f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate topics based on the no. of optimal topics which is '10' in this case \n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary,random_state=0)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dominant topic in each sentence of the text\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=filtered_list):\n",
    "    # Init output\n",
    "    sent_topics_list = []\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        if row:\n",
    "            topic_num, prop_topic = row[0]  # Dominant topic\n",
    "            wp = ldamodel.show_topic(topic_num)\n",
    "            topic_keywords = \", \".join([word for word, prop in wp])\n",
    "            sent_topics_list.append([int(topic_num), round(prop_topic, 4), topic_keywords])\n",
    "    \n",
    "    sent_topics_df = pd.DataFrame(sent_topics_list, columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "    \n",
    "    # Add original text to the end of the output\n",
    "    sent_topics_df = pd.concat([sent_topics_df, pd.Series(texts, name='Text')], axis=1)\n",
    "    \n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=filtered_list)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for documents belonging to each topic\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "rows = 2  # Adjust as needed\n",
    "cols = 5  # Adjust as needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 10))\n",
    "fig.suptitle(\"Word Clouds for All Topics\", fontsize=16)\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for topic_num in range(num_topics):\n",
    "    # Filter the DataFrame for the current topic\n",
    "    df_topic = df_dominant_topic[df_dominant_topic['Dominant_Topic'] == topic_num] \n",
    "    text = df_topic['Text'].to_string()\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(max_words=50, background_color='#FFFFFF').generate(text)\n",
    "    \n",
    "    # Display word cloud\n",
    "    axes[topic_num].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[topic_num].axis(\"off\")\n",
    "    axes[topic_num].set_title(f\"Topic {topic_num}\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4fad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find words that provide significant differentiation between the topics selected based on the multinomial logistic regression model using the forward entry method.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def compute_chi_square(X, y):\n",
    "    chi2_values = []\n",
    "    p_values = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        X_word = X[:, i].reshape(-1, 1)  # Single word feature\n",
    "        model = LogisticRegression(solver='lbfgs')\n",
    "        model.fit(X_word, y)\n",
    "        \n",
    "        # Compute log-likelihood\n",
    "        log_likelihood_full = np.sum(np.log(model.predict_proba(X_word)[range(len(y)), y]))\n",
    "        \n",
    "        # Null model (without this word)\n",
    "        null_model = LogisticRegression(solver='lbfgs')\n",
    "        null_model.fit(np.ones((X.shape[0], 1)), y)\n",
    "        log_likelihood_null = np.sum(np.log(null_model.predict_proba(np.ones((X.shape[0], 1)))[range(len(y)), y]))\n",
    "        \n",
    "        # Chi-square statistic\n",
    "        chi2_stat = 2 * (log_likelihood_full - log_likelihood_null)\n",
    "        p_value = 1 - chi2.cdf(chi2_stat, df=1)\n",
    "        chi2_values.append(chi2_stat)\n",
    "        p_values.append(p_value)\n",
    "    \n",
    "    return chi2_values, p_values\n",
    "\n",
    "def select_significant_words(X, y, significance_level=0.05):\n",
    "    chi2_values, p_values = compute_chi_square(X, y)\n",
    "    significant_features = [i for i, p in enumerate(p_values) if p < significance_level]\n",
    "    return significant_features, chi2_values, p_values\n",
    "\n",
    "df_dominant_topic[\"Text2\"] = df_dominant_topic[\"Text\"].apply(lambda x: \" \".join(eval(x)) if isinstance(x, str) else \" \".join(x))\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_dominant_topic['Text2']).toarray()\n",
    "y = np.array(df_dominant_topic['Dominant_Topic'])\n",
    "\n",
    "# Perform feature selection\n",
    "selected_indices, chi2_values, p_values = select_significant_words(X, y, significance_level=0.05)\n",
    "selected_words = [vectorizer.get_feature_names_out()[i] for i in selected_indices]\n",
    "\n",
    "# Print results\n",
    "for i, word in enumerate(vectorizer.get_feature_names_out()):\n",
    "    print(f\"Word: {word}, Chi-square: {chi2_values[i]:.4f}, p-value: {p_values[i]:.4f}\")\n",
    "\n",
    "print(\"Selected words providing significant differentiation:\", selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the percentage of memos within each topic that contained each distinguishing word.\n",
    "vectorizer = CountVectorizer(vocabulary=selected_words, binary=True)\n",
    "X = vectorizer.fit_transform(df_dominant_topic['Text2'])\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "word_presence = pd.DataFrame(X.toarray(), columns=selected_words)\n",
    "df_dominant_topic = df_dominant_topic.reset_index(drop=True)\n",
    "df_word_presence = pd.concat([df_dominant_topic[['Dominant_Topic']], word_presence], axis=1)\n",
    "\n",
    "# Calculate proportions: For each topic, compute the percentage of memos containing each word\n",
    "word_proportions = df_word_presence.groupby('Dominant_Topic').mean().T*100\n",
    "\n",
    "# Display results\n",
    "print(word_proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a4849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
