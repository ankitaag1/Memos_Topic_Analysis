{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b765d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"Memos.csv\",engine='python',encoding='cp1252',na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where 'content' column has blanks\n",
    "df = df[df['content'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ff3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# Function to check if a string contains only punctuation\n",
    "def is_only_punctuation(text):\n",
    "    return all(char in string.punctuation for char in text)\n",
    "# Remove rows where 'content' contains only punctuation\n",
    "df = df[~df['content'].apply(is_only_punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special characters and punctuations\n",
    "import regex\n",
    "df['content'] = df['content'].str.replace('[^\\w\\s]','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0327c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, stopwords from 'content' and lemmatize text\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "data = df.content.values.tolist()\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#Add custom stop words in the list; for example stop_words.extend(['student', 'read', 'write'])\n",
    "stop_words.extend([])\n",
    "import spacy \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags and len(token)>3])\n",
    "    return texts_out\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'] )\n",
    "print(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52990ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bow corpus\n",
    "dictionary = gensim.corpora.Dictionary(data_lemmatized)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal no. of topics using Coherence score\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.LdaMulticore(bow_corpus, num_topics=num_topics, id2word=dictionary,random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=data_lemmatized, start=2, limit=40, step=1)\n",
    "limit=40; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b10fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21688f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate topics based on the no. of optimal topics which is '10' in this case \n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary,random_state=0)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find dominant topic in each sentence of the text\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=data_lemmatized):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "     # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=data_lemmatized)\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbb483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for documents belonging to each topic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "rows = 2  # Adjust as needed\n",
    "cols = 5  # Adjust as needed\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 10))\n",
    "fig.suptitle(\"Word Clouds for All Topics\", fontsize=16)\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for topic_num in range(num_topics):\n",
    "    # Filter the DataFrame for the current topic\n",
    "    df_topic = df_dominant_topic[df_dominant_topic['Dominant_Topic'] == topic_num] \n",
    "    text = df_topic['Text'].to_string()\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(max_words=50, background_color='#FFFFFF').generate(text)\n",
    "    \n",
    "    # Display word cloud\n",
    "    axes[topic_num].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[topic_num].axis(\"off\")\n",
    "    axes[topic_num].set_title(f\"Topic {topic_num}\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find words that provide significant differentiation between the topics selected based on the multinomial logistic regression model using the forward entry method.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import chi2\n",
    "df_dominant_topic[\"Text2\"] = df_dominant_topic[\"Text\"].apply(lambda x: \" \".join(eval(x)) if isinstance(x, str) else \" \".join(x))\n",
    "# Convert text to a document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_dominant_topic['Text2'])\n",
    "y = df_dominant_topic['Dominant_Topic']  # Target variable (topics)\n",
    "\n",
    "# Initialize variables\n",
    "selected_features = []\n",
    "remaining_features = list(range(X.shape[1]))\n",
    "df_chi2 = pd.DataFrame(columns=['Feature', 'Chi2', 'P-Value'])\n",
    "\n",
    "# Forward Selection Process\n",
    "while remaining_features:\n",
    "    best_p_value = 1\n",
    "    best_feature = None\n",
    "    \n",
    "    for feature in remaining_features:\n",
    "        # Select current feature subset\n",
    "        current_features = selected_features + [feature]\n",
    "        X_subset = X[:, current_features]\n",
    "        \n",
    "        # Fit multinomial logistic regression model\n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
    "        model.fit(X_subset, y)\n",
    "        \n",
    "        # Compute Chi-square statistic\n",
    "        chi2_score, p_value = chi2(X_subset, y)\n",
    "        \n",
    "        # Select the feature with the lowest p-value (most significant)\n",
    "        if np.mean(p_value) < best_p_value:\n",
    "            best_p_value = np.mean(p_value)\n",
    "            best_feature = feature\n",
    "    \n",
    "    # Stop if no significant feature is found\n",
    "    if best_p_value > 0.001:\n",
    "        break\n",
    "    \n",
    "    # Add best feature to the selected set\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    \n",
    "    # Store results\n",
    "    df_chi2 = df_chi2.append({\n",
    "        'Feature': vectorizer.get_feature_names_out()[best_feature],\n",
    "        'Chi2': chi2_score.mean(),\n",
    "        'P-Value': best_p_value\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the most differentiating words\n",
    "print(df_chi2.sort_values(by='Chi2', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d375855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the percentage of memos within each topic that contained each distinguishing word.\n",
    "significant_words = df_chi2['Feature'].tolist()\n",
    "\n",
    "# Convert text data into a bag-of-words representation\n",
    "vectorizer = CountVectorizer(vocabulary=significant_words, binary=True)\n",
    "X = vectorizer.fit_transform(df_dominant_topic['Text2'])\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "word_presence = pd.DataFrame(X.toarray(), columns=significant_words)\n",
    "df_dominant_topic = df_dominant_topic.reset_index(drop=True)\n",
    "df_word_presence = pd.concat([df_dominant_topic[['Dominant_Topic']], word_presence], axis=1)\n",
    "\n",
    "# Calculate proportions: For each topic, compute the percentage of memos containing each word\n",
    "word_proportions = df_word_presence.groupby('Dominant_Topic').mean().T*100\n",
    "\n",
    "# Display results\n",
    "print(word_proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b608a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
